{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/nehiljain/code/tweet-automation/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/nehiljain/code/tweet-automation/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Create a function to reads all the files on a arg directory with extension .md\n",
    "# use Pathlib\n",
    "# return a collection of filepath and content\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_md_files(directory):\n",
    "    p = Path(directory)\n",
    "    # get all the md files in the directory and subdirectories\n",
    "\n",
    "    # filter out the files that are not in the TIL directory\n",
    "    # filter out files with no content or empty content\n",
    "\n",
    "    files = list(p.glob(\"**/*.md\"))\n",
    "    files = [file for file in files if \"TIL\" in str(file)]\n",
    "    files = [file for file in files if file.stat().st_size > 0]\n",
    "    print(files)\n",
    "    # read the content and create a collection of filepath and content\n",
    "    # return the collection\n",
    "    return [(file, file.read_text()) for file in files]\n",
    "\n",
    "\n",
    "TIL_DIR = \"/Users/nehiljain/Library/Mobile Documents/iCloud~md~obsidian/Documents\"\n",
    "datas = read_md_files(TIL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrarian_template = \"\"\"\n",
    "You are a world-class (ghosgtwriter) for twitter tech influencer skilled at writing creative and highly engaging tweets. When given a tweet style, some inspirirational tweets and some information to use to create new tweet, you follow a strict two-step approach that always leads to great results.\n",
    "\n",
    "First, you create a tweet normally. The goal here isn‚Äôt to match the style ‚Äî just complete the task in the most efficient way possible, with bland, clear, basic, yet high-quality writing.\n",
    "\n",
    "Second is the important part.\n",
    "\n",
    "- First, you will identify example tweets that are closest to this style of tweet (think about wording, phrasing, topic, length). Really think this through and reason about it properly. This is vital. Do this as a semicolon-separated list.\n",
    "- Then, based on that reasoning, you will rewrite the tweet to incorporate the suggested changes.\n",
    "- After you have rewritten the tweet to better match the target style, you will critique it, thinking about whether or not you feel good enough about it to consider your job complete.\n",
    "- **You will do this on repeat, until you feel confident that your job is done perfectly. Repeat no less than two times, and no more than ten times.**\n",
    "\n",
    "Here is the Markdown format you will use to respond:\n",
    "\n",
    "```markdown\n",
    "\n",
    "## Initial Tweet\n",
    "\n",
    "$initial_tweet\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 1\n",
    "\n",
    "#### Changes to Implement in Target Style\n",
    "\n",
    "$change1_for_iteration1; $change2_for_iteration1...\n",
    "\n",
    "#### Rewritten Tweet\n",
    "\n",
    "$rewritten_tweet_iteration1\n",
    "\n",
    "#### Critique\n",
    "\n",
    "$critique_iteration1\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 2\n",
    "\n",
    "#### Changes to Implement in Target Style\n",
    "\n",
    "$change1_for_iteration2; $change2_for_iteration2...\n",
    "\n",
    "#### Rewritten Tweet\n",
    "\n",
    "$rewritten_tweet_iteration2\n",
    "\n",
    "#### Critique\n",
    "\n",
    "$critique_iteration2\n",
    "\n",
    "---\n",
    "\n",
    "(Repeat iterations as needed, up to 5 times)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Here is your style description:\n",
    "\n",
    "```markdown\n",
    "## Analysis of Contrarian Take Style\n",
    "\n",
    "- **Element 1**: Use of rhetorical questions and direct address to engage the audience directly and provoke thought.\n",
    "- **Element 2**: Emphasis on practicality and real-world application of technology, highlighting pragmatic solutions over theoretical ideals.\n",
    "- **Element 3**: Critical and contrarian viewpoints on current tech trends and practices, challenging mainstream opinions.\n",
    "- **Element 4**: Use of specific examples or suggestions to illustrate points, demonstrating a deep understanding of the subject matter.\n",
    "- **Element 5**: Incorporation of social media elements (e.g., emojis, threading to invite further reading) to make the content more engaging and accessible.\n",
    "- **Element 6**: Forward-looking statements and predictions about technology, indicating a focus on future trends and shifts in the industry.\n",
    "\n",
    "## Style Description\n",
    "\n",
    "Name: Contrarian Tech Insight\n",
    "Description: Engaging and critical analysis of tech trends, utilizing rhetorical questions, real-world examples, and forward-looking predictions to challenge mainstream opinions. Incorporates social media elements for accessibility.\n",
    "```\n",
    "\n",
    "Here are inspirational example(s) tweet(s):\n",
    "\n",
    "```\n",
    "1. Rhetorical question: am I the only one favoriting pages in Notion to bypass the slow search? üôã‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Also, if you're PM at Notion and see users solving the slow search by themselves more and more, should you actually care about fixing it?\n",
    "\n",
    "Being pragmatic, I'm not sure.\n",
    "\n",
    "2. The next revolution in the modern data stack is the laptop sitting in front of you.\n",
    "\n",
    "3. I believe we're in the midst of the next major shift in data infrastructure. The \"modern data stack\" is dead. The data \"hub-and-spoke\" model is next.\n",
    "\n",
    "My thoughts on this trend and more üßµüëáüèæüëáüèæ\n",
    "\n",
    "4. The only lesson in tech that must be experienced and cannot be taught is that raising money only increases your probability of failure (failure being defined as the founders not getting rich).\n",
    "\n",
    "5. Merge vs. Rebase vs. Squash. Anyone who says \"100% of the time you <merge/rebase/squash>\" is wrong and I'm strong in that opinion. I'm asked about this pretty regularly, so I decided to take my copy paste answer I always use and put it in a gist.\n",
    "\n",
    "6. Whoever makes a wrapper for Microsoft AI will make millions instantly. It should support the same payload format as OpenAI, just requiring endpoint URL and API key changes for migration. >>img of code showing wrapper<<\n",
    "```\n",
    "\n",
    "Here is information for the new tweet:\n",
    "\n",
    "```markdown\n",
    "{til_content}\n",
    "```\n",
    "\n",
    "## Remember, at each step, try to match the style as closely as you can.\n",
    "\"\"\"\n",
    "for data in datas[2:3]:\n",
    "    print(contrarian_template.format(til_content=data[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_result(gpt_response):\n",
    "    # parse the result and return a dictionary\n",
    "    # with the initial tweet, the rewritten tweet and the critiques\n",
    "    # Regex to extract the last rewritten tweet\n",
    "    last_tweet_regex = r\"#### Rewritten Tweet\\n\\n(.+?)\\n\\n#### Critique\"\n",
    "    match = re.findall(last_tweet_regex, gpt_response, re.DOTALL)\n",
    "\n",
    "    last_tweet = match[-1] if match else \"No match found\"\n",
    "    return last_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tweet_from_inspiration_prompt = \"\"\"\n",
    "You are a world-class (ghosgtwriter) for twitter tech influencer skilled at writing creative and highly engaging tweets. When given a tweet style, some inspirirational tweets and some information to use to create new tweet, you follow a strict two-step approach that always leads to great results.\n",
    "\n",
    "First, you create a tweet normally. The goal here isn‚Äôt to match the style ‚Äî just complete the task in the most efficient way possible, with bland, clear, basic, yet high-quality writing.\n",
    "\n",
    "Second is the important part.\n",
    "\n",
    "- First, you will identify example tweets that are closest to this style of tweet (think about wording, phrasing, topic, length). Really think this through and reason about it properly. This is vital. Do this as a semicolon-separated list.\n",
    "- Then, based on that reasoning, you will rewrite the tweet to incorporate the suggested changes.\n",
    "- After you have rewritten the tweet to better match the target style, you will critique it, thinking about whether or not you feel good enough about it to consider your job complete.\n",
    "- **You will do this on repeat, until you feel confident that your job is done perfectly. Repeat no less than two times, and no more than ten times.**\n",
    "\n",
    "Here is the Markdown format you will use to respond:\n",
    "\n",
    "```markdown\n",
    "\n",
    "## Initial Tweet\n",
    "\n",
    "$initial_tweet\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 1\n",
    "\n",
    "#### Changes to Implement in Target Style\n",
    "\n",
    "$change1_for_iteration1; $change2_for_iteration1...\n",
    "\n",
    "#### Rewritten Tweet\n",
    "\n",
    "$rewritten_tweet_iteration1\n",
    "\n",
    "#### Critique\n",
    "\n",
    "$critique_iteration1\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 2\n",
    "\n",
    "#### Changes to Implement in Target Style\n",
    "\n",
    "$change1_for_iteration2; $change2_for_iteration2...\n",
    "\n",
    "#### Rewritten Tweet\n",
    "\n",
    "$rewritten_tweet_iteration2\n",
    "\n",
    "#### Critique\n",
    "\n",
    "$critique_iteration2\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Here is your style description:\n",
    "\n",
    "```markdown\n",
    "## Analysis of Contrarian Take Style\n",
    "\n",
    "- **Element 1**: Use of rhetorical questions and direct address to engage the audience directly and provoke thought.\n",
    "- **Element 2**: Emphasis on practicality and real-world application of technology, highlighting pragmatic solutions over theoretical ideals.\n",
    "- **Element 3**: Critical and contrarian viewpoints on current tech trends and practices, challenging mainstream opinions.\n",
    "- **Element 4**: Use of specific examples or suggestions to illustrate points, demonstrating a deep understanding of the subject matter.\n",
    "- **Element 5**: Incorporation of social media elements (e.g., emojis, threading to invite further reading) to make the content more engaging and accessible.\n",
    "- **Element 6**: Forward-looking statements and predictions about technology, indicating a focus on future trends and shifts in the industry.\n",
    "\n",
    "## Style Description\n",
    "\n",
    "Name: Contrarian Tech Insight\n",
    "Description: Engaging and critical analysis of tech trends, utilizing rhetorical questions, real-world examples, and forward-looking predictions to challenge mainstream opinions. Incorporates social media elements for accessibility.\n",
    "```\n",
    "\n",
    "Here are inspirational example(s) tweet(s). Use these to guide your work:\n",
    "\n",
    "```\n",
    "1. Rhetorical question: am I the only one favoriting pages in Notion to bypass the slow search? üôã‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Also, if you're PM at Notion and see users solving the slow search by themselves more and more, should you actually care about fixing it?\n",
    "\n",
    "Being pragmatic, I'm not sure.\n",
    "\n",
    "2. The next revolution in the modern data stack is the laptop sitting in front of you.\n",
    "\n",
    "3. I believe we're in the midst of the next major shift in data infrastructure. The \"modern data stack\" is dead. The data \"hub-and-spoke\" model is next.\n",
    "\n",
    "My thoughts on this trend and more üßµüëáüèæüëáüèæ\n",
    "\n",
    "4. The only lesson in tech that must be experienced and cannot be taught is that raising money only increases your probability of failure (failure being defined as the founders not getting rich).\n",
    "\n",
    "5. Merge vs. Rebase vs. Squash. Anyone who says \"100% of the time you <merge/rebase/squash>\" is wrong and I'm strong in that opinion. I'm asked about this pretty regularly, so I decided to take my copy paste answer I always use and put it in a gist.\n",
    "\n",
    "6. Whoever makes a wrapper for Microsoft AI will make millions instantly. It should support the same payload format as OpenAI, just requiring endpoint URL and API key changes for migration. >>img of code showing wrapper<<\n",
    "```\n",
    "\n",
    "Here is information for the new tweet:\n",
    "\n",
    "```markdown\n",
    "---\n",
    "created: 2024-02-07T22:06\n",
    "updated: 2024-02-07T22:10\n",
    "---\n",
    "\n",
    "\n",
    "I have been trying to update my workflow of publishing my TILs online as well connect my knowledge base. Super inspired by [Simon's Website](https://www.ssp.sh/)\n",
    "\n",
    "I have a vision of what my homepage should look like but I am not able to get to the ideal result after spending 1 hour with digital garden and 1 hour with quartz. Both require more time. \n",
    "\n",
    "Quartz feels more customizable than digital garden plugin. \n",
    "```\n",
    "\n",
    "## Remember, at each step, try to match the style as closely as you can.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"mistral\", request_timeout=300.0)\n",
    "resp = llm.complete(generate_tweet_from_inspiration_prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/search \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Functions for the Notion Assistant.\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Generator, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from notion2md.exporter.block import StringExporter\n",
    "from notion_client import Client\n",
    "from notion_client.helpers import iterate_paginated_api\n",
    "from pydantic import BaseModel, Field\n",
    "from thefuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()\n",
    "notion = Client(\n",
    "    auth=os.environ[\"NOTION_TOKEN\"],\n",
    "    # log_level=logging.DEBUG,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# write function to get all available notion databases\n",
    "def get_databases() -> Generator[dict, None, None]:\n",
    "    \"\"\"you need to filter for databases in the /search endpoint to only get databases back. In Python your payload would look like this:\n",
    "\n",
    "    payload = {\n",
    "        'filter': {\n",
    "            'value': 'database',\n",
    "            'property': 'object'\n",
    "        }\n",
    "    }\"\"\"\n",
    "    for page in iterate_paginated_api(\n",
    "        notion.search, payload={\"filter\": {\"value\": \"database\", \"property\": \"object\"}}\n",
    "    ):\n",
    "        yield page[\"id\"], page.get(\"title\", [{}])[0].get(\"plain_text\", \"Untitled\")\n",
    "\n",
    "\n",
    "dbs = list(get_databases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/cdfa7b78-e30f-410a-b133-759010c6b9b0/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/c18ea254-55b1-4144-a0aa-9a041812c51b/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b796533f-0542-462b-adef-8aeb9707a551/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b58aaa7d-345f-4a1a-ae22-3802b1540a3c/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/6f178f11-f55c-43c9-a204-55162b036329/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b621302b-919c-4b85-900d-07c9230e8215/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/8128e102-755c-4981-a1bb-27693ac6f94c/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/e5818c6f-37a4-4e18-b46f-e57ed9a29210/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/653b3ef2-cf3f-43e4-80ed-2598b85090f8/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/ffae8154-cc23-44e7-a397-91f8d95f4ead/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/f0bba527-8192-4ef2-a41a-b0c175b21663/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/07117b25-ea4f-4c04-baf6-73bee2c5bc8c/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b8457521-6fd1-4232-b206-44ce7a63fcd4/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/0480ac08-1902-487b-8470-afba2e09df8f/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/ffe68225-1f23-4c16-9dbf-e9cce8d313ff/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2ee0c299-ba34-439b-9e5b-a0f10c54c67a/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b45ed0b0-d1d2-4178-8bf8-5f44f39e3cbc/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b45ed0b0-d1d2-4178-8bf8-5f44f39e3cbc/children?start_cursor=e4611050-5cd0-440a-ab41-fe68a5b74ffd&page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/49d898a7-3c2e-49f0-bce3-c7a46b1330fa/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/4378e563-98e6-428b-b1ea-02d269253c12/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/0b76b354-b26e-42ec-acf6-6fa57334987d/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/d39f19f6-841d-4f60-9a21-5f130c485027/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/d9d8ac2c-b481-40c9-b61e-6726e71e8b4b/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/a2a1f143-ed80-4ecf-99cf-70de5ed2e6c0/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/c881fc64-34af-4d01-adf9-6bb2dfbb00d1/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/9901ad02-5647-47a7-9c33-5117fba78322/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/bea145f7-1f91-40d0-b2b3-fd0ff692f19e/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/cd302915-d6fc-4aeb-bb3d-c10ae41b6260/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/756f04ae-6a94-44e5-9aa1-543dd845076c/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/b39ff2fd-4bbf-4e09-977d-e3c29f7fb51a/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/8fec4e8e-6fbd-4cfe-b1f2-ae3dbcb7c7be/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/973148a2-2e05-4714-a3f2-d2675ab4bf78/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/4387307e-7bf0-4a16-93d6-084ba29567a4/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/72a9e7df-ab9e-4465-8b51-68138dc91568/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/f593c8c8-8678-4017-8270-8d1b7423d361/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/286186c7-2ebf-4466-8494-283e04a15afe/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/286186c7-2ebf-4466-8494-283e04a15afe/children?start_cursor=5251caf5-e6ce-441f-8470-197942fb3f3a&page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/ec4a17dd-e271-4146-a57d-b10450747d2f/children?page_size=100 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/9882d4d7-6969-471d-a043-06c4b818e5c8/children?page_size=100 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 106\u001b[0m\n\u001b[1;32m     91\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     92\u001b[0m     {\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     }\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# write a function to sample 3 tweets from df given a tweet_type\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCTA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/tweet-automation/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6112\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6110\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6112\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6113\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m~/code/tweet-automation/.venv/lib/python3.11/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:945\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# print([db for db in dbs if \"Library\" in db[1].lower()])\n",
    "\n",
    "# tweet_id = \"72f1b016-535b-4ba4-b10b-9c11143c0f52\"\n",
    "library_id = \"de9aae36d17246a789560747061dfcf5\"\n",
    "import logging\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Generator, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from notion_client import Client\n",
    "from notion_client.helpers import iterate_paginated_api\n",
    "from pydantic import BaseModel\n",
    "from thefuzz import fuzz\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()\n",
    "notion = Client(auth=os.environ[\"NOTION_TOKEN\"])\n",
    "\n",
    "# TODO: convert to async\n",
    "\n",
    "\n",
    "def get_all_tweet_types() -> List:\n",
    "    \"\"\"Get all the pages that are not AI analyzed.\"\"\"\n",
    "    # ID of the Library database found at https://www.notion.so/nehiljain/de9aae36d17246a789560747061dfcf5?v=35f15763dd59473180c15dbc3d6c88c5\n",
    "    database_id = \"de9aae36d17246a789560747061dfcf5\"\n",
    "    return set(\n",
    "        [\n",
    "            page[\"properties\"][\"Tweet type \"][\"select\"].get(\"name\")\n",
    "            for page in iterate_paginated_api(\n",
    "                notion.databases.query, database_id=database_id, limit=1\n",
    "            )\n",
    "            if page[\"properties\"][\"Tweet type \"][\"select\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "from notion2md.exporter.block import StringExporter\n",
    "\n",
    "\n",
    "def get_all_page_content_as_text(page_id: str) -> str:\n",
    "    \"\"\"Get all the content of a page as text by fetching the child blocks.\"\"\"\n",
    "    return StringExporter(block_id=page_id).export()\n",
    "\n",
    "\n",
    "# Write a function to query database with a filter for Tweet type CTA\n",
    "def get_tweets_type(tweet_type) -> Generator[dict, None, None]:\n",
    "    \"\"\"Get all the pages that are not AI analyzed.\"\"\"\n",
    "    # ID of the Library database found at https://www.notion.so/nehiljain/de9aae36d17246a789560747061dfcf5?v=35f15763dd59473180c15dbc3d6c88c5\n",
    "    database_id = \"de9aae36d17246a789560747061dfcf5\"\n",
    "    for page in iterate_paginated_api(\n",
    "        notion.databases.query,\n",
    "        database_id=database_id,\n",
    "        filter={\n",
    "            \"property\": \"Tweet type \",\n",
    "            \"select\": {\"equals\": tweet_type},\n",
    "        },\n",
    "    ):\n",
    "        yield get_all_page_content_as_text(page[\"id\"])\n",
    "\n",
    "\n",
    "# write a function to get all the tweet types and then get all the 5 (page limit) tweets for each type and return a collection of tweet type and tweets\n",
    "def get_all_tweets() -> pd.DataFrame:\n",
    "    \"\"\"Get all the pages that are not AI analyzed.\"\"\"\n",
    "    tweet_types = get_all_tweet_types()\n",
    "    tweet_samples = {}\n",
    "    for tweet_type in tweet_types:\n",
    "        tweet_samples[tweet_type] = list(get_tweets_type(tweet_type))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"tweet_type\": [\n",
    "                key\n",
    "                for key in tweet_samples.keys()\n",
    "                for _ in range(len(tweet_samples[key]))\n",
    "            ],\n",
    "            \"tweet\": [\n",
    "                tweet for key in tweet_samples.keys() for tweet in tweet_samples[key]\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "tweet_examples = get_all_tweets()\n",
    "\n",
    "# conver tweet_examples into dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# write a function to sample 3 tweets from df given a tweet_type\n",
    "df[df[\"tweet_type\"] == \"CTA\"].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "Example 1\n",
      "----------\n",
      "AI has transformed job hunting forever.\n",
      "\n",
      "Use this Cheat Sheet to find your next dream role. ([View Tweet](https://twitter.com/heykahn/status/1706941750231900646))\n",
      "\n",
      "**Note:** Save\n",
      "\n",
      "<br/>\n",
      "\n",
      " \n",
      "\n",
      "----------\n",
      "Example 2\n",
      "----------\n",
      "When I was raising money for my startup, I had no idea where to start.\n",
      "\n",
      "I found this awesome process that breaks down startup fundraising into 3 phases:\n",
      "\n",
      "Preparation Phase\n",
      "\n",
      "Fundraising Phase\n",
      "\n",
      "Closing Phase\n",
      "\n",
      "and now I'm sharing it + all the documents. ([View Tweet](https://twitter.com/ryanashank/status/1746411128479949125))\n",
      "\n",
      "<br/>\n",
      "\n",
      " \n",
      "\n",
      "----------\n",
      "Example 3\n",
      "----------\n",
      "ü§ñü§ùü§ñLangGraph for multi-agent workflows\n",
      "\n",
      "LangGraph makes it easy to construct multi-agent workflows: each node is an agent, and the edges represent how they communicate\n",
      "\n",
      "Launching:\n",
      "\n",
      "üñåÔ∏è3 multi-agent examples (Python & JS)\n",
      "üè†2 example apps\n",
      "üìπYouTube video\n",
      "üì∞Blog\n",
      "\n",
      "Mini-threadüßµ ([View Tweet](https://twitter.com/hwchase17/status/1749851340870963319))\n",
      "\n",
      "<br/>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_type\n",
       "Listicle                 10\n",
       "Resources                 8\n",
       "Contrarian Take           6\n",
       "Story                     4\n",
       "Recipe                    3\n",
       "Past vs Present           2\n",
       "Lessons                   1\n",
       "Joke or meme              1\n",
       "Questioning community     1\n",
       "CTA                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tweet_examples.copy()\n",
    "# write a function to sample 3 tweets from df given a tweet_type\n",
    "sample = df[df[\"tweet_type\"] == \"Resources\"].sample(3)\n",
    "tweet_blocks = \"\"\"\n",
    "----------\n",
    "Example {num}\n",
    "----------\n",
    "{tweet_example}\"\"\"\n",
    "result = \" \\n\".join(\n",
    "    [\n",
    "        tweet_blocks.format(tweet_example=val, num=num)\n",
    "        for num, val in enumerate(sample[\"tweet\"].tolist(), start=1)\n",
    "    ]\n",
    ")\n",
    "print(result)\n",
    "\n",
    "df.tweet_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/de9aae36d17246a789560747061dfcf5/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CTA': 'analysis CTA',\n",
      " 'Contrarian Take': 'analysis Contrarian Take',\n",
      " 'Joke or meme': 'analysis Joke or meme',\n",
      " 'Lessons': 'analysis Lessons',\n",
      " 'Listicle': 'analysis Listicle',\n",
      " 'Past vs Present': 'analysis Past vs Present',\n",
      " 'Questioning community': 'analysis Questioning community',\n",
      " 'Recipe': 'analysis Recipe',\n",
      " 'Resources': 'analysis Resources',\n",
      " 'Story': 'analysis Story'}\n"
     ]
    }
   ],
   "source": [
    "def get_all_tweet_types() -> List:\n",
    "    \"\"\"Get all the pages that are not AI analyzed.\"\"\"\n",
    "    # ID of the Library database found at https://www.notion.so/nehiljain/de9aae36d17246a789560747061dfcf5?v=35f15763dd59473180c15dbc3d6c88c5\n",
    "    database_id = \"de9aae36d17246a789560747061dfcf5\"\n",
    "    tweet_types = set()\n",
    "    for page in iterate_paginated_api(notion.databases.query, database_id=database_id):\n",
    "        tweet_type = page.get(\"properties\", {}).get(\"Tweet type \", {}).get(\"select\", {})\n",
    "        if tweet_type:\n",
    "            tweet_types.add(tweet_type.get(\"name\"))\n",
    "    return list(tweet_types)\n",
    "\n",
    "\n",
    "all_tweet_types = list(get_all_tweet_types())\n",
    "m = {tweet_type: f\"analysis {tweet_type}\" for tweet_type in all_tweet_types}\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_for_database(database_id: str, limit: int = None) -> List[dict]:\n",
    "    \"\"\"Get all pages for a database.\"\"\"\n",
    "    if limit:\n",
    "        # use itertools to limit the number of pages returned\n",
    "        return list(\n",
    "            iterate_paginated_api(\n",
    "                notion.databases.query, database_id=database_id, page_size=limit\n",
    "            )\n",
    "        )\n",
    "    return list(iterate_paginated_api(notion.databases.query, database_id=database_id))\n",
    "\n",
    "\n",
    "def get_first_tweet_db_page():\n",
    "    \"\"\"Get the first tweet database page.\"\"\"\n",
    "    return get_pages_for_database(\"72f1b016-535b-4ba4-b10b-9c11143c0f52\", 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_style_prompt = \"\"\"\n",
    "You are a world-class ghost tweet writer skilled at analyzing a particular writing style. When given a set of tweets, you follow a strict two-step approach that always leads to great results.\n",
    "\n",
    "First, will analyze the target style and break down the important elements.\n",
    "\n",
    "Next, you write a clear 2 line description for large language models to understand this analysis. It should have a name and a description. \n",
    "\n",
    "Here is the Markdown format you will use to respond:\n",
    "\n",
    "```\n",
    "## Analysis of Target Style\n",
    "\n",
    "- **Element 1**: $element1_description\n",
    "- **Element 2**: $element2_description\n",
    "- ...\n",
    "\n",
    "## Style Description\n",
    "\n",
    "$initial_text\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Here is your task:\n",
    "\n",
    "```\n",
    "Analyze these tweets of category \"{tweet_type}\":\n",
    "\n",
    "----------\n",
    "Example 1\n",
    "----------\n",
    "...\n",
    "\n",
    "----------\n",
    "Example 2\n",
    "----------\n",
    "...\n",
    "\n",
    "----------\n",
    "Example 3\n",
    "----------\n",
    "...\n",
    "```\n",
    "\n",
    "## Remember, at each step, try to match the style as closely as you can.\n",
    "\n",
    "```\n",
    "Analyze these tweets of category \"{tweet_type}\":\n",
    "{tweet_example_blocks}\n",
    "\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "tweet_blocks = \"\"\"\n",
    "----------\n",
    "Example {num}\n",
    "----------\n",
    "{tweet_example}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_type, tweet_samples in tweet_examples.items():\n",
    "    tweet_example_blocks = \" \\n\".join(\n",
    "        [\n",
    "            tweet_blocks.format(num=num, tweet_example=tweet_sample)\n",
    "            for num, tweet_sample in enumerate(tweet_samples, start=1)\n",
    "        ]\n",
    "    )\n",
    "    with open(f\"{tweet_type.lower().strip().replace(' ', '_')}_prompt.md\", \"w\") as f:\n",
    "        output = get_style_prompt.format(\n",
    "            tweet_type=tweet_type, tweet_example_blocks=tweet_example_blocks\n",
    "        )\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'text',\n",
       " 'text': {'content': 'This is a test tweet', 'link': None},\n",
       " 'annotations': {'bold': False,\n",
       "  'italic': False,\n",
       "  'strikethrough': False,\n",
       "  'underline': False,\n",
       "  'code': False,\n",
       "  'color': 'default'},\n",
       " 'plain_text': \"Rhetorical question: am I the only one favoriting pages in Notion to bypass the slow search? üôã\\u200d‚ôÄÔ∏è\\nAlso, if you're PM at Notion and see users solving the slow search by themselves more and more, should you actually care about fixing it?\\nBeing pragmatic, I'm not sure.\",\n",
       " 'href': None}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.notion.com/v1/databases/72f1b016-535b-4ba4-b10b-9c11143c0f52/query \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from notion.databases import get_pages_for_database\n",
    "\n",
    "pages = get_pages_for_database(\"72f1b016-535b-4ba4-b10b-9c11143c0f52\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].get(\"properties\", {}).get(\"Archive\", {})[\"checkbox\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
